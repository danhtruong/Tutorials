---
title: "Non-negative Matrix Factorization"
output:
  md_document:
    variant: markdown_github
---

## Non-negative Matrix Factorization

In this post, I will be discussing Non-negative Matrix Factorization (NMF). NMF is a low-rank approximation algorithm that discovers latent features in your data. It is similar to PCA in the sense that they both reduce high-dimensional data into lower dimensions for better understanding of the data. The major difference is that PCA projects data onto a subspace that maximizes variability for the discovered features, while NMF discovers non-negative features that are additive in nature.

NMF is formally defined as:

<pre><pre>$$V \approx{WH}$$</pre></pre>

where <pre>$V$</pre> is a non-negative matrix and both <pre>$W$</pre> and <pre>$H$</pre> are unique and non-negative matrices. In other words, the matrix <pre>$V$</pre> is factorized into two matrices <pre>$W$</pre> and <pre>$H$</pre>, where <pre>$W$</pre> is the features matrix or the basis and <pre>$H$</pre> is the coefficient matrix. Typically, this means that <pre>$H$</pre> represents a coordinate system that uses <pre>$W$</pre> to reconstruct <pre>$V$</pre>. We can consider that <pre>$V$</pre> is a linear combination of column vectors of <pre>$W$</pre> using the coordinate system in <pre>$H$</pre>, <pre>$v_{i} = Wh_{i}$</pre>.

## Solving for NMF

Here, I will describe two algorithms to solve for NMF using iterative updates of <pre>$W$</pre> and <pre>$H$</pre>. First, we will consider the cost function. A cost function is a function that quantifies or measures the error between the predicted values and the expected values. The Mean Squared Error (MSE), or L2 loss is one of the most popular cost functions in linear regressions. Given an linear equation <pre>$y = mx + b$</pre>, MSE is:


<pre><pre>$$MSE = \frac{1}{N}\Sigma_{i=1}^{n}(y_{i}-(mx_{i}+b))^{2}$$</pre></pre>


For the cost function in NMF, we use a similar function called the Frobenius Norm. It is defined as:

<pre><pre>$$||A||_{F} = \sqrt{\Sigma_{i=1}^{m}\Sigma_{j=1}^{n} |a_{ij}|^{2}}$$</pre></pre>

In the case of NMF, we are using the square of the Forbenius norm to measure how good of an approximation <pre>$WH$</pre> is for <pre>$V$</pre>.

<pre><pre>$$||V - WH||_{F}^{2} = \Sigma_{i,j}(V - WH)^{2}_{ij}$$</pre></pre>

## Optimization

We can see that as <pre>$WH$</pre> approaches <pre>$V$</pre>, then the equation will slowly converge to zero. Therefore, the optimization can be defined as the following:

Minimize <pre>$||V - WH||_{F}^{2}$</pre> with respect to <pre>$W$</pre> and <pre>$H$</pre>, subject to the constraints <pre>$W,H \ge 0$</pre>

In the paper by Lee & Seung, they introduced the multiplicative update rule to solve for NMF. Please see their original paper for details on the proof. Essentially the update causes the function value to be non-increasing to converge to zero.

<pre><pre>$$H_{ik} \leftarrow H_{ik}\frac{(W^{T}V)_{ik}}{(W^{T}WH)_{ik}}$$</pre></pre>


<pre><pre>$$W_{kj} \leftarrow W_{kj}\frac{(VH^{T})_{kj}}{(WHH^{T})_{kj}}$$</pre></pre>

## Multiplicative Update Rule

Here we will implement NMF using the multiplicative update rule. To get started, make sure you have installed both [R](https://www.r-project.org/) and [RStudio](https://rstudio.com/). In addition, we will also be using the package [NMF](https://cran.r-project.org/web/packages/NMF/index.html) to benchmark our work. 

Here I write a function to solve for NMF using the multiplicative update rule. I added a `delta` variable to the denominator update rule to prevent division by zero. `K` specifies the column length of <pre>$W$</pre> and the row length of <pre>$H$</pre>. `K` can also be considered as the number of hidden features we are discovering in <pre>$V$</pre>. `K` is less than <pre>$n$</pre> in a <pre>$n \times m$</pre> matrix. After iterating for `x` number of `steps`, the function returns a `list` containing `W` and `H` for <pre>$W$</pre> and <pre>$H$</pre> respectively. 

```{r}
nmf_mu <- function(R, K, delta = 0.001, steps = 5000){
  N = dim(R)[1]
  M = dim(R)[2]
  K = N
  W <- rmatrix(N,K) #Random initialization of W
  H <- rmatrix(K,M) #Random initialization of H
  for (step in 1:steps){
    W_TA = t(W) %*% R
    W_TWH = t(W) %*% W %*% H + delta
    for (i in 1:dim(H)[1]){
      for (j in 1:dim(H)[2]){
        H[i, j] = H[i, j] * W_TA[i, j] / W_TWH[i, j] #Updating H
      }
    }
    RH_T = R %*% t(H)
    WHH_T = W %*% H %*% t(H) + delta
    for (i in 1:dim(W)[1]){
      for (j in 1:dim(W)[2]){
        W[i, j] = W[i, j] * RH_T[i, j] / WHH_T[i, j] #Updating W
      }
    }
  }
  list <- list('W'=W, 'H'=H)
  return(list)
}
```

Let's initialize a random <pre>$n \times m$</pre> matrix and test our function.
```{r}
require(NMF)
R <- rmatrix(5,6)
R
```

```{R}
nmf_mu_results <- nmf_mu(R)
cat('\fMatrix W is:\n')
print(nmf_mu_results$W)
cat('Matrix H is:\n')
print(nmf_mu_results$H)
```

Let's see if we can reconstruct our original matrix and compare it to the `nmf` function.
```{r}
R
nmf_mu_results$W %*% nmf_mu_results$H
```

## Comparing to nmf()

We get the same results using the `nmf` function with the `lee` method. 
```{r}
nmf <- nmf(R, dim(R)[1], method = 'lee')
basis(nmf) %*% coefficients(nmf)
```

## Stochastic Gradient Descent Method

Now we will take a look at another method of implementing NMF. This one is called Stochastic Gradient Descent (SGD). A gradient descent is a first-order iterative optimization algorithm to finding a local minimum for a function that is differentiable. In fact, we used the Block Coordinate Descent in the multiplicative update rule. In SGD, we take the derivative of the cost function like before. However, we will now be focusing on taking the derivative of each variable, setting them to zero or lower, solving for the feature variables, and finally updating each feature. We also add a regularization term in the cost function to control for over fitting. 

<pre><pre>$$||V - WH||_{F}^{2} = \Sigma_{i,j}(V - WH)^{2}_{ij}$$</pre></pre>

<pre><pre>$$e_{ij}^{2} = \Sigma_{i,j}(v_{ij}- \hat v_{ij})^{2} = (v_{ij} - \Sigma_{k=1}^{K} w_{ik}h_{kj})^{2}$$</pre></pre>

<pre><pre>$$e_{ij}^{2}  = (v_{ij} - \Sigma_{k=1}^{K}w_{ik}h_{kj})^{2} + \lambda\Sigma_{k=1}^{K}(||W||^{2} + ||H||^{2})$$</pre></pre>

<pre>$\lambda$</pre> is used to control the magnitudes of <pre>$w$</pre> and <pre>$h$</pre> such that they would provide a good approximation of <pre>$v$</pre>. We will update each feature with each sample. We choose a small <pre>$\lambda$</pre>, such as 0.01. The update is given by the equations below:

<pre><pre>$$w_{ik} \leftarrow w_{ik} - \eta \frac{\partial}{\partial w_{ik}}e_{ij}^{2}$$</pre></pre>

<pre><pre>$$h_{kj} \leftarrow h_{kj} - \eta \frac{\partial}{\partial h_{kj}}e_{ij}^{2}$$</pre></pre>

<pre>$\eta$</pre> is the learning rate and modifies the magnitude that we update the features. We first solve for <pre>$\frac{\partial}{\partial h_{kj}}e_{ij}^{2}$</pre>.

Using the chain rule, <pre>$\frac{\partial}{\partial h_{kj}}(v_{ij} - \Sigma_{k=1}^{K}w_{ik}h_{kj}) = \frac{\partial u^{2}}{\partial v} \frac{\partial u}{\partial v}$</pre>, where <pre>$u = (v_{ij} - \Sigma_{k=1}^{K}w_{ik}h_{kj}) and \frac{\partial u^{2}}{\partial v} = 2u$</pre>

<pre><pre>$$ \frac{\partial}{\partial h_{kj}}e_{ij}^{2} = 2(v_{ij} - \Sigma_{k=1}^{K}w_{ik}h_{kj}) \frac{\partial}{\partial h_{kj}}(v_{ij} - \Sigma_{k=1}^{K}w_{ik}h_{kj}) + 2\lambda h_{kj} $$</pre></pre>

<pre><pre>$$ \frac{\partial}{\partial h_{kj}}e_{ij}^{2} = -2e_{ij}w_{ik} + 2\lambda h_{kj} $$</pre></pre>

The final update rules for both <pre>$W$</pre> and <pre>$H$</pre>:

<pre><pre>$$ \frac{\partial}{\partial h_{kj}}e_{ij}^{2} = -2e_{ij}w_{ik} + 2\lambda h_{kj} $$</pre></pre>

<pre><pre>$$ \frac{\partial}{\partial w_{ik}}e_{ij}^{2} = -2e_{ij}h + 2\lambda w_{ik} $$</pre></pre>

```{r}
frob_norm <- function(M){
  norm = 0
  for (i in 1:dim(M)[1]){
    for (j in 1:dim(M)[2]){
      norm = norm + M[i,j] ** 2
    }
  }
  return(sqrt(norm))
}
nmf_sgd <- function(A,steps = 50000, lam = 1e-2, lr = 1e-3){
  N = dim(A)[1]
  M = dim(A)[2]
  K = N
  W <- rmatrix(N,K)
  H <- rmatrix(K,M)
  for (step in 1:steps){
    R =  A - W %*% H 
    dW = R %*% t(H) - W*lam
    dH = t(W) %*% R - H*lam
    W = W + lr * dW
    H = H + lr * dH
    if (frob_norm(A - W %*% H) < 0.01){
      print(frob_norm(A - W %*% H))
      break
    }
  }
  list <- list(W, t(H))
  return(list)
}
```

```{r}
nmf_sgd_results <- nmf_sgd(R)
R
```

The reconstructed method using our NMF function looks like this:

```{r}
nmf_sgd_results[[1]] %*% t(nmf_sgd_results[[2]])
```

As you can see, it is a near approximation of the original matrix. In another post, I will go into detail the differences between each dimensional reduction technique. See below for more information on NMF. 


## Additional Resources

  -[Algorithms for non-negative matrix factorization](https://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization.pdf)
  
  -[Non-negative matrix factorization](https://github.com/hpark3910/nonnegative-matrix-factorization)
  
  -[Non-negative matrix factorization](https://www.almoststochastic.com/2013/06/nonnegative-matrix-factorization.html)
  -[NMF from scratch using SGD](https://github.com/carriexu24/NMF-from-scratch-using-SGD)
  -[Python Matrix Factorization](https://albertauyeung.github.io/2017/04/23/python-matrix-factorization.html)


