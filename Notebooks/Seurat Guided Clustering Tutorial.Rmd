---
title: "Seurat-Guided-Clustering-Tutorial"
output:
  md_document:
    variant: markdown_github
---

While the vignette on the Seurat website already provides good instructions, I will be using this to give additional thoughts and details that could help beginners to Seurat. In addition, I will provide some recommendations on the workflow as well.

<h2>Loading the files</h2>

The first thing the tutorial asks you to do is download the raw data from [10x Genomics](https://s3-us-west-2.amazonaws.com/10x.files/samples/cell/pbmc3k/pbmc3k_filtered_gene_bc_matrices.tar.gz). The raw data contains three key files within it: barcodes.tsv, genes.tsv, and matrix.mtx. You will also notice they are contained in a folder called hg19. That simply means the counts were generated using the hg19 genome (Genome Reference Consortium Human Build 37 (GRCh37)) as the reference transcriptome. 

Let's load in each of the files and take a look at them. 
```{r}
barcodes <- read.table("data/filtered_gene_bc_matrices/hg19/barcodes.tsv")
head(barcodes)
```
```{r}
genes <- read.delim("data/filtered_gene_bc_matrices/hg19/genes.tsv", header=FALSE)
head(genes)
```

```{r}
library(Matrix)
mat <- readMM(file = "data/filtered_gene_bc_matrices/hg19/matrix.mtx")
mat[1:5, 1:10]
```

You will notice when we take a look at the matrix file, it contains `.`. These are used when no count is detected rather using a value of 0. This is called a sparse matrix to reduce memory and increase computational speed. It is pretty much standard to work using sparse matrices when dealing with single-cell data.

<h2>Generating the Seurat Object</h2>

Next, we will generate a `Seurat` object based on the files we loaded up earlier. 
```{r}
library(dplyr)
library(Seurat)
library(patchwork)

# Load the PBMC dataset
pbmc.data <- Read10X(data.dir = "data/filtered_gene_bc_matrices/hg19/")
# Initialize the Seurat object with the raw (non-normalized data).
pbmc <- CreateSeuratObject(counts = pbmc.data, project = "pbmc3k", min.cells = 3, min.features = 200)
```


```{r}
pbmc
```

<h2>Pre-processing the data</h2>

Now we will pre-process the data and perform quality control on the cells. There are a couple of metrics that are used within the community.

- The number of unique genes detected in each individual cell.
    - Low-quality cells or empty droplets will often have very few genes. 
    - Sometimes this could be ambient mRNA that is detected.
    - Cell doublets or multiplets may exhibit an aberrantly high gene count. There are some additional packages that can be used to detect doublets.
- The percentage of reads that map to the mitochondrial genome
    - Low-quality / dying cells often exhibit extensive mitochondrial contamination.
    - This is often calculated by searching for genes containing `MT-`
```{r}
# The [[ operator can add columns to object metadata. This is a great place to stash QC stats
pbmc[["percent.mt"]] <- PercentageFeatureSet(pbmc, pattern = "^MT-")
```

Let's take a look at the metadata which includes some of the QC metrics. 

- `nCount_RNA` is the number of unique counts in each cell.
- `nFeature_RNA` is the number of unique genes in each cell.
- `percent.mt` is the mitochondrial mapping that we just calculated.
```{r}
head(pbmc@meta.data, 5)
```

Seurat recommends a threshold for filtering for the QC metrics.

- Cells are filtereed for unique feature counts over 2,500 or less than 200
- Cells are filtereed for <5% mitochondrial counts
```{r}
# Visualize QC metrics as a violin plot
VlnPlot(pbmc, features = c("nFeature_RNA", "nCount_RNA", "percent.mt"), ncol = 3)
```

We can take a look and see that the unique counts and feature are correlated. In addition, we see that low counts appear to correlate with high mitochondrial mapping percentage.
```{r}
# FeatureScatter is typically used to visualize feature-feature relationships, but can be used
# for anything calculated by the object, i.e. columns in object metadata, PC scores etc.

plot1 <- FeatureScatter(pbmc, feature1 = "nCount_RNA", feature2 = "percent.mt")
plot2 <- FeatureScatter(pbmc, feature1 = "nCount_RNA", feature2 = "nFeature_RNA")
plot1 + plot2
```

I find that the density plots provide a better visualization of the distributions in case you may have a bimodal distribution.
```{r}
# Visualize QC metrics as ridge plots
RidgePlot(pbmc, features = c("nFeature_RNA", "nCount_RNA", "percent.mt"), ncol =1)
```
Let's use an adaptive threshold rather than fixed threshold. This provides a more elegant method of detecting the thresholds rather than by eye using the graphs. First, we assume that most of the cells are high-quality. Next, we then identify cells that are outliers using the median absolute deviation (MAD) from the median value of each metric for all cells. Outliers will be beyond the MAD threshold and we can specify higher or lower than the threshold. 

To identify outliers based on unique genes and counts, we use log-transformed `nCount_RNA` and `nFeature_RNA` that are more than 3 MADs above and below the median. We will be using the `scater` library for this.
```{r}
library(scater)
qc.nCount_RNA <- isOutlier(pbmc$nCount_RNA, log=TRUE, type="both")
qc.nFeature_RNA  <- isOutlier(pbmc$nFeature_RNA, log=TRUE, type="both")
```

We can see the thresholds that are identified in both QC metric.
```{r}
attr(qc.nCount_RNA , "thresholds")
```

```{r}
attr(qc.nFeature_RNA, "thresholds")
```

We can also do the same for `percent.mt`. In this case, we want to remove cells above the MAD.
```{r}
qc.percent.mt <- isOutlier(pbmc$percent.mt,  type="higher")
attr(qc.percent.mt, "thresholds")
```

Another reason to use adapative thresholds is if your data contains multiple batches. In this case, you would detect the QC threshold for each batch rather than for your entire dataset. It makes little sense to a single threshold from a data set with samples from multiple batches. While there are no batches in this current data set, you would need to include a `batch` parameter when using the `isOutlier` function.

Let's continue on with the original tutorial and use the thresholds that were fixed but feel free to try the adaptive thresholds.
```{r}
pbmc <- subset(pbmc, subset = nFeature_RNA > 200 & nFeature_RNA < 2500 & percent.mt < 5)
```

<h2>Normalizing the data</h2>

After filtering out the low quality cells from the dataset, the next step is to normalize the data. By default, `Seurat` employs a global-scaling normalization method “LogNormalize” that normalizes the feature expression measurements for each cell by diviing by the total expression, multiplies the result by a scale factor (10,000 by default), and then log-transforms the result to obtain the normalized data. 
```{r}
pbmc <- NormalizeData(pbmc)
```

It is common to identify highly variable features or genes for dimensional reduction. By reducing your analysis to the highly variable genes, you account for most of the biolgical heterogeneity or factors in your data and hopefully ignore a majority of the noise while reducing computational work and time. As such, the highly variable genes should enable us to isolate the real biological signals.
```{r}
pbmc <- FindVariableFeatures(pbmc, selection.method = "vst", nfeatures = 2000)

# Identify the 10 most highly variable genes
top10 <- head(VariableFeatures(pbmc), 10)

# plot variable features with and without labels
plot1 <- VariableFeaturePlot(pbmc) + theme(legend.text = element_text(size = 6))
plot2 <- LabelPoints(plot = plot1, points = top10, repel = TRUE) + theme(legend.text = element_text(size = 6))
plot1 + plot2
```

<h1>Scaling the data</h1>

Next, we will need to scale the data. This is a standard pre-processing step prior dimensional reduction, like PCA, which I discussed in a previous post. 

- All gene expression will be centered to 0.
- Scales the expression of each gene to have a variance of 1 so all genes have equal contributions

As an additional step, we typically scale the highly variable genes as these are the genes that will be used for dimensional reductiobn.
```{r}
pbmc <- ScaleData(pbmc, features = VariableFeatures(object = pbmc)) #VariableFeatures is used to call the highly variable genes from the object.
```

<h2>Perform linear dimensional reduction </h2>

PCA is already built into `Seurat` and can be called the function `RunPCA`. 
```{r}
pbmc <- RunPCA(pbmc, features = VariableFeatures(object = pbmc), verbose = FALSE)
# Examine and visualize PCA results a few different ways
print(pbmc[["pca"]], dims = 1:5, nfeatures = 5)
```

We can visualize the first two prinicpal components. 
```{r}
DimPlot(pbmc, reduction = "pca")
```

Let's inspect the contribution of each of the prinicpal components. Typically only the the principal components containing a majority of the variance is used. This can be estimated by using an 'elbow plot' and observing where there is a large drop off in variance. 
```{r}
ElbowPlot(pbmc)
```

It may be difficult to estimate by visualization so you can use the second derivative which should find the maximum change in the slope. The following code should provide another method for estimating the drop off. 
```{r}
variance <- pbmc@reductions[["pca"]]@stdev
dy <- -diff(range(variance))
dx <- length(variance) - 1
l2 <- sqrt(dx^2 + dy^2)
dx <- dx/l2
dy <- dy/l2
dy0 <- variance - variance[1]
dx0 <- seq_along(variance) - 1
parallel.l2 <- sqrt((dx0 * dx)^2 + (dy0 * dy)^2)
normal.x <- dx0 - dx * parallel.l2
normal.y <- dy0 - dy * parallel.l2
normal.l2 <- sqrt(normal.x^2 + normal.y^2)
below.line <- normal.x < 0 & normal.y < 0
if (!any(below.line)) {
    length(variance)
} else {
    which(below.line)[which.max(normal.l2[below.line])]
}
```

While the largest drop off does occur at PC3, we can see that there in additional drop off at around PC9-10, suggesting that the majority of true signal is captured in the first 10 PCs.

<h2>Cluster the cells</h2>

To aid in summarizing the data for easier interpretation, scRNA-seq is often clustered to empirically define groups of cells within the data that have similar expression profiles. This generates discrete groupings of cells for the downstream analysis. `Seurat` uses a graph-based clustering approach.  There are additional approaches such as k-means clustering or hierachical clustering. 

The major advantage of graph-based clustering compared to the other two methods is its scalability and speed. Simply, `Seurat` first constructs a KNN graph based on the euclidean distance in PCA space. Each node is a cell that is connected to its nearest neighbors. Edges, which are the lines between the neighbors, are weighted based on the similarity between the cells involved, with higher weight given to cells that are more closely related. A refinement step  (Jaccard similarity) is used to refine the edge weights between any two cells based on the shared overlap in their local neighborhoods. 

Here we use the first 10 PCs to construct the neighbor graph.
```{r}
pbmc <- FindNeighbors(pbmc, dims = 1:10)
```

Next, we can apply algorithms to identify “communities” of cells. There are two main community detection algorithm, Louvain[Louvain](https://perso.uclouvain.be/vincent.blondel/research/louvain.html) and the improved version Leiden[Leiden](https://www.nature.com/articles/s41598-019-41695-z). We use the default Louvain while controlling the `resolution` parameter to adjust the number of clusters.

```{r}
pbmc <- FindClusters(pbmc, resolution = 0.5)
```

<h2> Visualization of 2D Embeddsings </h2>

The high-dimensional space can be embedded in 2D using either tSNE and UMAP to visualize and explore the datasets. These embeddings attempt to summarize all of the data using a 2D graph to organize the data to better interpret the relationships between the cells. Cells that are more similar to one another should localize within the graph. Different types of embeddings will relay different information on the relationship between cells. UMAP is more recommended to be more faithful to the global connectivity of the manifold than tSNE, while tSNE might preserve the local structure more. 


```{r}
# If you haven't installed UMAP, you can do so via reticulate::py_install(packages =
# 'umap-learn')
# Here we generate a UMAP embedding.
pbmc <- RunUMAP(pbmc, dims = 1:10)
# Here we generate a tSNE embedding.
pbmc <- RunTSNE(pbmc, dims = 1:10)
```

We can visualize all three of the dimensional reductions that we generated. You will notice that UMAP and tSNE generate better visualizations as they are more adept at interpreting non-linear data. PCA is linear technique and does not capture the non-linear relationship of gene expression profiles very well. 
```{r}
# note that you can set `label = TRUE` or use the LabelClusters function to help label
# individual clusters
plot1 <- DimPlot(pbmc, reduction = "umap")
plot2 <- DimPlot(pbmc, reduction = "tsne")
plot3 <- DimPlot(pbmc, reduction = "pca")
plot1 +  plot2 + plot3 + plot_layout(nrow = 2)
```

We can see that cells are clustered closer toether while also providing some global relationship between the clusters within the UMAP embedding. tSNE generates some similar clusters with the same local relationship but the global relationship can not be estimated using tSNE. In addition, tSNE does not scale with larger data sets. The PCA captures highest variation within the first two PCs but does not include information from the additional PCs. You can see here the failure of PCA to resolve the differences between the clusters unlike UMAP and tSNE. 

<h2>Finding Marker Genes</h2>

To interpret our clusters, we can identify the genes and markers that drive separation of the clusters. `Seurat` canfind these markers via differential expression. By default, `FindAllMarkers` uses Wilcoxon rank-sum (Mann-Whitney-U) test to find DEs. You can choose which test by modifying the `test.use` parameter. If you have blocking factors (i.e., batches), you can include it using the `latent.vars` parameter.

```{r}
# find markers for every cluster compared to all remaining cells, report only the positive ones
pbmc.markers <- FindAllMarkers(pbmc, only.pos = TRUE, min.pct = 0.25, logfc.threshold = 0.25)
pbmc.markers %>% group_by(cluster) %>% top_n(n = 2, wt = avg_logFC)
```

You can export the markers as a 'csv' for analysis outside of R.
```{r}
write.csv(pbmc.markers, 'pbmc.markers.csv')
```


We can visualize some of these markers using `VlnPlot`.
```{r}
VlnPlot(pbmc, features = c("MS4A1", "CD79A"))
```

```{r}
# you can plot raw counts as well
VlnPlot(pbmc, features = c("NKG7", "PF4"), slot = "counts", log = TRUE)
```

We can visualize the localization of these markers in a UMAP embedding by using `FeaturePlot`. 
```{r}
FeaturePlot(pbmc, features = c("MS4A1", "GNLY", "CD3E", "CD14", "FCER1A", "FCGR3A", "LYZ", "PPBP", 
    "CD8A"))
```

We can generate a dot plot to visualize the markers.
```{r}
DotPlot(pbmc, features = c("MS4A1", "GNLY", "CD3E", "CD14", "FCER1A", "FCGR3A", "LYZ", "PPBP", 
    "CD8A")) + theme(axis.text.x = element_text(angle = 90))
```

Another common method of visualization is to generate a heatmap. We can use `DoHeatmap` to see the top 10 markers per cluster. 
```{r}
top10 <- pbmc.markers %>% group_by(cluster) %>% top_n(n = 10, wt = avg_logFC)
DoHeatmap(pbmc, features = top10$gene) + NoLegend()
```

<h2>Assigning cell type identity to clusters</h2>

We can use canonical markers to easily match the unbiased clustering to known cell types based on the table below:

Cluster ID | Markers| Cell Type 
---|---|---|
0 |	IL7R, CCR7 |	Naive CD4+ T 
1	| IL7R, S100A4	| Memory CD4+
2	| CD14, LYZ	CD14+ | Mono
3	| MS4A1	| B
4	| CD8A	CD8+ | T
5	| FCGR3A, MS4A7	FCGR3A+ | Mono
6	| GNLY, NKG7	|  NK
7	| FCER1A, CST3	| DC
8	| PPBP	| Platelet

```{r}
new.cluster.ids <- c("Naive CD4 T", "Memory CD4 T", "CD14+ Mono", "B", "CD8 T", "FCGR3A+ Mono", 
    "NK", "DC", "Platelet")
names(new.cluster.ids) <- levels(pbmc)
pbmc <- RenameIdents(pbmc, new.cluster.ids)
DimPlot(pbmc, reduction = "umap", label = TRUE, pt.size = 0.5) + NoLegend()
```

We can also use annotation packages like `SingleR` to perform automatic annotation. You can find more information [here](https://bioconductor.org/packages/release/bioc/html/SingleR.html). We initialize the Humany Primary Cell Atlas data.

```{r}
library(SingleR)
ref <- HumanPrimaryCellAtlasData()
```

We can see some of the labels within this reference set.
```{r}
as.data.frame(colData(ref))
```

We use our `ref` reference to annotate each cell in `pbmc` via the `SingleR` function. 
```{r}
pbmc.sce <- as.SingleCellExperiment(pbmc) #convert to SingleCellExperiment
pred.pbmc <- SingleR(test = pbmc.sce, ref = ref, assay.type.test=1,
    labels = ref$label.main)
```

Each row of the output DataFrame contains prediction results for each cell. 
```{r}
pred.pbmc
```
```{r}
# Summarizing the distribution:
table(pred.pbmc$labels)
```

Let's see how it compared to our manual annotations. First, we will add the predicted labels as new metadata.
```{r}
pbmc <- AddMetaData(pbmc, pred.pbmc$labels, col.name = 'SingleR.Annotations')
plot1 <- DimPlot(pbmc, reduction = "umap", label = TRUE, pt.size = 0.5) + NoLegend() + ggtitle('Manual Annotations')
plot2 <- DimPlot(pbmc, reduction = "umap", group.by = 'SingleR.Annotations', label = TRUE, pt.size = 0.5) + NoLegend() + ggtitle('SingleR Annotations')
plot1 + plot2
```

We can see that the predicted labels and manually anotated labels match up pretty well. Your choice of reference data and parameters can be used to further fine-tine the predicted labels. Here we used a bulk reference data set but a single-cell reference data that is well-annotated set could be used as well. 

Finally, always make sure to save your data.
```{r}
saveRDS(pbmc, file = "pbmc3k_final.rds")
```

