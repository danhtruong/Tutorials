---
title: Writing the code for PCA
output:
  md_document:
    variant: markdown_github
---

```{r opts, echo = FALSE}
knitr::opts_chunk$set(
  fig.path = "images/PCA_scratch_files/"
)
```

## Principal Component Analysis

In this post, we will extend what we did in the previous PCA tutorial by doing it from scratch in R. We will be focusing on Singular Value Decomposition (SVD), which is a tool for matrix factorization. SVD states that any matrix <pre>$M \in A^{m \times n} $</pre> can be decomposed into the following:

<pre><pre>$$M = U \Sigma V^{T}$$</pre></pre>

where <pre>$U$</pre> is an <pre>$m \times m$</pre> unitary matrix, <pre>$\Sigma$</pre> is a diagonal <pre>$m \times n$</pre> matrix with non-negative real numbers on the diagonal, and <pre>$V$</pre> is an <pre>$n \times n$</pre> unitary matrix. If <pre>$M$</pre> is real, then <pre>$U$</pre> and <pre>$V$</pre> are orthogonal matrices, or uncorrelated. <pre>$U$</pre> and <pre>$V$</pre> are equal to the eigenvectors of <pre>$MM^{t}$</pre> and <pre>$M^{t}M$</pre>, respectively. The entries in <pre>$\Sigma$</pre> are singular values of <pre>$M$</pre>. The number of non-zero singular values is equal to the rank of <pre>$M$</pre>. If you recall, the rank of a matrix is defined as the maximum number of linearly independent columns in the matrix. As it relates to PCA, these are the number of Principal Components. 

## Covariance matrix

Performing SVD on the covariance matrix of <pre>$M$</pre> is essentially PCA. We will be using the Iris data set.

```{r}
data(iris)
D <- data.matrix(iris[,c(1:4)]) #Here we are only taking the first four columns as the last one is categorical data.
```

The covariance is defined as follows:

<pre><pre>$$cov_{x,y} = \frac{\Sigma(x_{i}-\overline{x})(y_{i}-\overline{y})}{N-1}$$</pre></pre>

First, we will center our data by subtracting each value by the of their respective column.

```{r}
xcenter = colMeans(D) #Find the mean of the column
print(xcenter)

D_centered <- D - rep(xcenter, rep.int(nrow(D), ncol(D)))
head(D_centered)
```

We will multiply the transposed form of our matrix by our matrix followed dividing by <pre>$N-1$</pre>, where N is number of rows. We will compare this to the R function `cov()`

```{r}
covariance = t(D_centered) %*% (D_centered)
covariance = covariance / (dim(D_centered)[1] - 1)
covariance
cov(D_centered)

cat('\n Are the values the same:', all(round(cov(D_centered) - covariance, 3) == 0)) 
```

Next, we will solve for <pre>$MM^{t}$</pre> and <pre>$M^{t}M$</pre> to find <pre>$U$</pre> and <pre>$V$</pre>.

```{r}
MTM <- t(covariance) %*% covariance
MTM.e <- eigen(MTM)
V <- MTM.e$vectors
V
```

```{r}
MMT <- covariance %*% t(covariance)
MMT.e <- eigen(MMT)
U <- MMT.e$vectors
U
```

Note that here we find that <pre>$U$</pre> and <pre>$V$</pre> are equivalent. This is a special case because we are using the covariance matrix. Now we find the singular values <pre>$\Sigma$</pre>, which are the square roots of the non-zero eigenvalues of <pre>$MM^{t}$</pre> and <pre>$M^{t}M$</pre>.

```{r}
sigma <- sqrt(MMT.e$values)
sigma <- sigma * diag(length(sigma))
sigma
```

## Singular Value Decomposition

Now recall the equation for SVD.

<pre><pre>$$M = U \Sigma V^{T}$$</pre></pre>

Now that, we have the decomposed values, we can recompose them using matrix multiplication to recover our covariance matrix of the Iris data set.

```{r}
M <- U %*% sigma %*% t(V)
M
covariance

cat('\n Are the values the same:', all(round(M - covariance, 3) == 0)) 
```

PCA is an orthogonal projection. To generate, we project our `D_centered` matrix using the rotation matrix `V`.

```{r}
pc <- D_centered %*% V
head(pc)
```

## Comparing to prcomp()

```{r}
pca_prcomp <- prcomp(D_centered, center = F)
head(pca_prcomp$x)
cat('\n Comparing our calculations to the R function prcomp:\n\n')
cat('\n Do we get the same rotation matrix:',  all(round(pca_prcomp$rotation - V, 3) == 0)) 
cat('\n Are the principal components the same:', all(round(pca_prcomp$x - pc, 3) == 0)) 
```

It looks like something is wrong with our. Upon closer inspection, it appears the third column has the opposite sign, which is a multiple of <pre>$-1$</pre>. Recall that <pre>$Au = \lambda u$</pre>, where <pre>$A$</pre> is a vector, <pre>$u$</pre> is the eigenvector, and <pre>$lambda$</pre> is the eigenvalue.

If <pre>$Au = \lambda u$</pre>, then <pre>$A(ku) = Akv = k \lambda u = \lambda (ku)$</pre>. Therefore, any multiple of <pre>$u$</pre> is also an eigenvector of <pre>$A$</pre>.
```{r}
V
pca_prcomp$rotation
```

```{r}
V[,3]  <- -V[,3]
pc[,3]  <- -pc[,3]
cat('\n Comparing our calculations to the R function prcomp:\n\n')
cat('\n Do we get the same rotation matrix:',  all(round(pca_prcomp$rotation - V, 3) == 0)) 
cat('\n Are the principal components the same:', all(round(pca_prcomp$x - pc, 3) == 0)) 
```


## Additional Resources

  - [PCA Analysis](https://www.datacamp.com/community/tutorials/pca-analysis-r)

  - [PCA done from scratch with R](https://medium.com/@ravikalia/pca-in-matrix-based-frameworks-9719e29cf7e6)

  - [Singular value decomposition](PCA done from scratch with R)


